{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3MdtNYC82-1",
    "outputId": "a8074fae-9686-412a-d5c2-c3566fe36ec5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# For loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uFhIprZmf7U5",
    "outputId": "30b38ac3-20db-4a02-c6c8-e55bbf85d82a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "a73c37f7ae224f06abffa703a343d85c",
       "pip_warning": {
        "packages": [
         "datasets"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "dc7a92f092724a47a492e5c2b4221249",
      "23ce43639efb4aee86929c375e11c6c7",
      "0e532099685b41f291b6a45be4e6b171",
      "a142930fa25f4ed19d396b947d767c60",
      "055399040cd242f2aba9b093ab5b737d",
      "bb5f63a125dd4dafb34b49edf0dd88e1",
      "9a0fa092807249feb6cf76fd7d0dee0f",
      "5a7e6436267347d4a175fbdbf8409fc4",
      "e29829fb0dcb46b397c759b973de4d7f",
      "47ae852af2f640d6860c06fc69d12792",
      "c13476cb11cf4d979d9f23e97a03d7f4",
      "57b45735750746148922a058e57f7021",
      "8d29bedb42964aa78ba0f1150e6d39e5",
      "f1cfc7b74bb041f098f91b5b607fe2f9",
      "78c0c075c43744dab34086539ef81455",
      "9a2d5124ce99427bb42ec9d873fc09a1",
      "9625368710814fef8e21515992d793c9",
      "9b50c64c4e934fb3acec69598c05ec96",
      "2b4dc7dff3ea415d84390b55b7231342",
      "c97ccc3c3b2b4d968de945a31f843af3",
      "c4f762ec1355403899518a9023266813",
      "f862c6ce1ee94dacb3289d7c414aaea3",
      "86c1c0a877274ee5b271974c53eb9f69",
      "2c92865150e14056ac436ac0a9d393a6",
      "b34620ecea444da1af27c47a639a33b2",
      "5b709c97f51a4b3d9b127f7d284834a9",
      "a18784f69f044e52a5cf8df632881cc9",
      "41acf225c8e14dbfa5601a1ddca046c8",
      "f2cde2d03a164926ba8cdc798c4d33f4",
      "a367ec5407cd42e4b61c9146ea033e46",
      "8d40c63d0e5a4e96952d9d44984f2545",
      "cb2818bf591749b8b5a17f847f1d523c",
      "9fb8811dbb594f97824c29b690192089",
      "e5a9c5dbf9f54b76b1009f943e6311bd",
      "4dcb9184498746d28a2a4296d93d983b",
      "e4e0738d86db41c794908df272059362",
      "3b4c02842a5b4ffeae6a9ca418fd8dce",
      "a21acc35ca574240ba6e0cee8e2c1ace",
      "1d75f59c89184566bea9ab30e45defc3",
      "78d135e83d6a4e81be18343e0922835f",
      "5cd754407ee641baab7acc23c6cb2cd9",
      "cf694abce8e84adb8093c421a24be309",
      "612f71e504be4de5ba6d650b2470880e",
      "72d6e1045d33481392d986dcf7a66be0",
      "3532bc4c0102446ca151a96f2928fd54",
      "4f54c94067cd4b96859dd4b5841410de",
      "0d377627743b43e298523e0c7554713f",
      "fff46241313d4c55b97ee525199720d3",
      "4d0973dbc8c2427b8fe408543c0d0227",
      "1e87595d95474b4284444755930d0e69",
      "5ba42eff3eb94c358f559ef28a9156dc",
      "e28b163a9cb048eca3f2c7c841bad0a7",
      "3235c065d5cc46399b8eb94f1f506095",
      "c68a76f674964df6a09848a2107219af",
      "aa675b0d89a746b2b5cabcc7aec46156",
      "a95a4a305a674c08aba739dd17d25734",
      "d89ba51c97a14366aa59cff80cdfc22a",
      "350243cdc1b74bf7a479fbe6e4ce8ac6",
      "1a1ca172bccb4acf8654bfdcd29622a0",
      "cad95fdd7cba43a7b72157345158ea02",
      "46b5a928030a4ed3b0231f13d1b62df9",
      "9c296572743843c9ad7da97c50f64f94",
      "c4bdd6ffc3ca4384a23dd480e48118ef",
      "1679fcc875b84af08edd41db606a6f89",
      "81db3262f78f4f21b6d580236faf7b4f",
      "94e3e0c38f054080ab62ba0dfdabaf09"
     ]
    },
    "id": "xxnHQKHt9z3-",
    "outputId": "26908053-b6e5-42f1-90e9-1f450296e2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Download and check the shape of the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7a92f092724a47a492e5c2b4221249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/327k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b45735750746148922a058e57f7021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c1c0a877274ee5b271974c53eb9f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/334k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a9c5dbf9f54b76b1009f943e6311bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3532bc4c0102446ca151a96f2928fd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a4a305a674c08aba739dd17d25734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded! Total training examples: 1000000\n",
      "Sample English: The time now is 05:08 ....\n",
      "Sample French: The time now is 05:05 ....\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"opus100\"\n",
    "language_pair = \"en-fr\"\n",
    "\n",
    "print(\"\\n2. Download and check the shape of the dataset\")\n",
    "dataset = load_dataset(\"opus100\", language_pair )\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(f\"\\nDataset loaded! Total training examples: {len(train_data)}\")\n",
    "print(f\"Sample English: {train_data[0]['translation']['en'][:60]}...\")\n",
    "print(f\"Sample French: {train_data[0]['translation']['fr'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "frjaL96N-LEV"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"['\\\",\\.\\?\\!\\-]\", \"\", text)  # Remove basic punctuation\n",
    "    text = re.sub(r\"[^a-zA-Zéèêëàâäôöûüç\\s]\", \"\", text)  # Keep French letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AoxbP6q-LXC",
    "outputId": "a6a3fc04-af44-4e79-e1bf-cc17a9580374"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:58<00:00, 16957.19it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 17268.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data cleaning complete!\n",
      "Example cleaned English: the time now is...\n",
      "Example cleaned French: the time now is...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_src = []\n",
    "train_tgt = []\n",
    "\n",
    "for sample in tqdm(train_data):\n",
    "    src_text = clean_text(sample['translation']['en'])\n",
    "    tgt_text = clean_text(sample['translation']['fr'])\n",
    "    train_src.append(src_text)\n",
    "    train_tgt.append(tgt_text)\n",
    "\n",
    "# Process test data\n",
    "test_src = []\n",
    "test_tgt = []\n",
    "\n",
    "for sample in tqdm(test_data):\n",
    "    src_text = clean_text(sample['translation']['en'])\n",
    "    tgt_text = clean_text(sample['translation']['fr'])\n",
    "    test_src.append(src_text)\n",
    "    test_tgt.append(tgt_text)\n",
    "\n",
    "print(\"\\nData cleaning complete!\")\n",
    "print(f\"Example cleaned English: {train_src[0][:60]}...\")\n",
    "print(f\"Example cleaned French: {train_tgt[0][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYfi_yno-iiq",
    "outputId": "547dd8e7-f66b-4bbf-f0d5-b94ce52ad418"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rj5GzaDp-Ojt",
    "outputId": "0cf4bf4e-3b60-4e7d-b2c0-159190828dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete!\n",
      "Example tokenized English: ['the', 'time', 'now', 'is']...\n",
      "Example tokenized French: ['the', 'time', 'now', 'is']...\n",
      "\n",
      "5. Build vocabulary for source and target languages\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Simple word tokenization\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "train_src_tokens = [tokenize(text) for text in train_src]\n",
    "train_tgt_tokens = [tokenize(text) for text in train_tgt]\n",
    "test_src_tokens = [tokenize(text) for text in test_src]\n",
    "test_tgt_tokens = [tokenize(text) for text in test_tgt]\n",
    "\n",
    "print(\"\\nTokenization complete!\")\n",
    "print(f\"Example tokenized English: {train_src_tokens[0][:10]}...\")\n",
    "print(f\"Example tokenized French: {train_tgt_tokens[0][:10]}...\")\n",
    "\n",
    "print(\"\\n5. Build vocabulary for source and target languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "w_-RNid2-aTB"
   },
   "outputs": [],
   "source": [
    "def build_vocab(token_lists, max_vocab_size=10000):\n",
    "    \"\"\"Build vocabulary from token lists\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for tokens in token_lists:\n",
    "        word_counts.update(tokens)\n",
    "\n",
    "    most_common = word_counts.most_common(max_vocab_size - 4)\n",
    "\n",
    "    vocab = {\n",
    "        '<PAD>': 0,\n",
    "        '<SOS>': 1,\n",
    "        '<EOS>': 2,\n",
    "        '<UNK>': 1\n",
    "    }\n",
    "\n",
    "    for idx, (word, _) in enumerate(most_common):\n",
    "        vocab[word] = idx + 4\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qlvfxft-1nN",
    "outputId": "4bf5d493-470b-4d73-a87f-c7a8bf1602bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source vocab size: 10000\n",
      "Target vocab size: 10000\n",
      "Sample source vocab items: [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 1), ('the', 4), ('of', 5), ('and', 6), ('to', 7), ('in', 8), ('a', 9)]\n",
      "Sample target vocab items: [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 1), ('de', 4), ('la', 5), ('et', 6), ('les', 7), ('le', 8), ('des', 9)]\n"
     ]
    }
   ],
   "source": [
    "src_vocab = build_vocab(train_src_tokens)\n",
    "tgt_vocab = build_vocab(train_tgt_tokens)\n",
    "\n",
    "print(f\"\\nSource vocab size: {len(src_vocab)}\")\n",
    "print(f\"Target vocab size: {len(tgt_vocab)}\")\n",
    "print(f\"Sample source vocab items: {list(src_vocab.items())[:10]}\")\n",
    "print(f\"Sample target vocab items: {list(tgt_vocab.items())[:10]}\")\n",
    "\n",
    "idx_to_src = {idx: word for word, idx in src_vocab.items()}\n",
    "idx_to_tgt = {idx: word for word, idx in tgt_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nUhPF1gN-16J"
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, max_len=50):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = self.src_texts[idx]\n",
    "        tgt_tokens = self.tgt_texts[idx]\n",
    "\n",
    "        src_indices = [self.src_vocab.get(token, self.src_vocab['<UNK>']) for token in src_tokens]\n",
    "        tgt_indices = [self.tgt_vocab.get(token, self.tgt_vocab['<UNK>']) for token in tgt_tokens]\n",
    "\n",
    "        tgt_indices = [self.tgt_vocab['<SOS>']] + tgt_indices + [self.tgt_vocab['<EOS>']]\n",
    "\n",
    "        src_indices = self._pad_or_truncate(src_indices, self.src_vocab['<PAD>'])\n",
    "        tgt_indices = self._pad_or_truncate(tgt_indices, self.tgt_vocab['<PAD>'])\n",
    "\n",
    "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)\n",
    "\n",
    "    def _pad_or_truncate(self, sequence, pad_idx):\n",
    "        \"\"\"Pad or truncate sequence to max_len\"\"\"\n",
    "        if len(sequence) > self.max_len:\n",
    "            return sequence[:self.max_len]\n",
    "        else:\n",
    "            return sequence + [pad_idx] * (self.max_len - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zrtW2UmC_IAA"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 30\n",
    "train_dataset = TranslationDataset(train_src_tokens, train_tgt_tokens, src_vocab, tgt_vocab, max_seq_len)\n",
    "test_dataset = TranslationDataset(test_src_tokens, test_tgt_tokens, src_vocab, tgt_vocab, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cI4CSoB1_S8U"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8mGoP-x_ksd",
    "outputId": "10ab64d6-2b49-41ec-8b9e-2f2bab15c57c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample batch shapes - Source: torch.Size([64, 30]), Target: torch.Size([64, 30])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "src_batch, tgt_batch = sample_batch\n",
    "print(f\"\\nSample batch shapes - Source: {src_batch.shape}, Target: {tgt_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JYsdrEPJ_mia"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_dim, n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(hidden_dim, tgt_vocab_size)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_embedded = self.dropout(self.encoder_embedding(src))\n",
    "        _, (hidden, cell) = self.encoder_lstm(enc_embedded)\n",
    "\n",
    "        dec_embedded = self.dropout(self.decoder_embedding(tgt[:, :-1]))  # Remove EOS token\n",
    "        dec_output, _ = self.decoder_lstm(dec_embedded, (hidden, cell))\n",
    "\n",
    "        output = self.decoder_fc(dec_output)\n",
    "        return output\n",
    "\n",
    "    def predict(self, src, tgt_vocab, max_len=30, device='cpu'):\n",
    "        \"\"\"Generate translation for a single source sequence\"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        src = src.unsqueeze(0).to(device)\n",
    "        enc_embedded = self.encoder_embedding(src)\n",
    "        _, (hidden, cell) = self.encoder_lstm(enc_embedded)\n",
    "\n",
    "        tgt = torch.tensor([[tgt_vocab['<SOS>']]], device=device)\n",
    "        output_seq = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            dec_embedded = self.decoder_embedding(tgt)\n",
    "            dec_output, (hidden, cell) = self.decoder_lstm(dec_embedded, (hidden, cell))\n",
    "            output = self.decoder_fc(dec_output.squeeze(1))\n",
    "\n",
    "            next_token = output.argmax(1)\n",
    "            output_seq.append(next_token.item())\n",
    "\n",
    "            if next_token.item() == tgt_vocab['<EOS>']:\n",
    "                break\n",
    "\n",
    "            tgt = next_token.unsqueeze(0)\n",
    "\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1XCTcmwiDrop"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = Seq2SeqLSTM(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yckeqjhxDvWt",
    "outputId": "36b6690e-79dd-45dc-c96e-ef812b33beb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model created and moved to cuda!\n",
      "Seq2SeqLSTM(\n",
      "  (encoder_embedding): Embedding(10000, 256, padding_idx=0)\n",
      "  (encoder_lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (decoder_embedding): Embedding(10000, 256, padding_idx=0)\n",
      "  (decoder_lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (decoder_fc): Linear(in_features=512, out_features=10000, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel created and moved to {device}!\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q3fLhzTdDzZA"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BZbJV9n-D2md"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, clip=1.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, tgt in tqdm(loader, desc=\"Training\"):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)  # Remove SOS token\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cxHgjNzmD6m_"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(loader, desc=\"Evaluating\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uarGxzA6D9Nm",
    "outputId": "1a6b7982-3123-4009-b3ff-5d0152643332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [13:30<00:00, 19.29it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:00<00:00, 55.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.149 | Val Loss: 3.635\n",
      "Model saved!\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [13:32<00:00, 19.23it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:00<00:00, 55.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.475 | Val Loss: 3.326\n",
      "Model saved!\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [13:32<00:00, 19.23it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:00<00:00, 53.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.244 | Val Loss: 3.176\n",
      "Model saved!\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [13:32<00:00, 19.23it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:00<00:00, 55.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.110 | Val Loss: 3.080\n",
      "Model saved!\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15625/15625 [13:34<00:00, 19.18it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:00<00:00, 53.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.017 | Val Loss: 3.015\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QRSpljV6ydLm"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, tgt_vocab, idx_to_tgt, device, max_len=30):\n",
    "    \"\"\"Translate a single sentence with proper dimension handling\"\"\"\n",
    "    sentence = clean_text(sentence)\n",
    "    tokens = tokenize(sentence)\n",
    "\n",
    "    indices = [src_vocab.get(token, src_vocab['<UNK>']) for token in tokens]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [src_vocab['<PAD>']] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "\n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_emb = model.encoder_embedding(src_tensor)\n",
    "        encoder_out, (hidden, cell) = model.encoder_lstm(encoder_emb)\n",
    "\n",
    "        decoder_input = torch.tensor([[tgt_vocab['<SOS>']]], device=device)\n",
    "        output_seq = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            decoder_emb = model.decoder_embedding(decoder_input)\n",
    "            decoder_out, (hidden, cell) = model.decoder_lstm(decoder_emb, (hidden, cell))\n",
    "            output = model.decoder_fc(decoder_out.squeeze(1))\n",
    "\n",
    "            next_token = output.argmax(1).item()\n",
    "            if next_token == tgt_vocab['<EOS>']:\n",
    "                break\n",
    "\n",
    "            output_seq.append(next_token)\n",
    "            decoder_input = torch.tensor([[next_token]], device=device)\n",
    "\n",
    "    translated_tokens = [idx_to_tgt[idx] for idx in output_seq if idx not in (tgt_vocab['<SOS>'], tgt_vocab['<EOS>'])]\n",
    "    return ' '.join(translated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7yFlIJ2ygB7",
    "outputId": "4e77904d-53be-41f2-e15e-b3c34663d0eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Translations:\n",
      "EN: hello                          → FR: bonjour\n",
      "EN: how are you                    → FR: comment ça va\n",
      "EN: what is your name              → FR: comment sappelle votre nom\n",
      "EN: this is a good example         → FR: cest un exemple\n",
      "EN: the weather is nice today      → FR: la journée est bonne aujourdhui\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"hello\",\n",
    "    \"how are you\",\n",
    "    \"what is your name\",\n",
    "    \"this is a good example\",\n",
    "    \"the weather is nice today\"\n",
    "]\n",
    "\n",
    "print(\"Successful Translations:\")\n",
    "for sent in test_sentences:\n",
    "    try:\n",
    "        translation = translate_sentence(model, sent, src_vocab, tgt_vocab, idx_to_tgt, device)\n",
    "        print(f\"EN: {sent.ljust(30)} → FR: {translation}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with '{sent}': {str(e)}\")\n",
    "        print(\"Debug Info:\")\n",
    "        print(f\"- Input shape: {src_tensor.shape if 'src_tensor' in locals() else 'N/A'}\")\n",
    "        print(f\"- Vocab size: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiQIu3Yhs-OP"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Path to your notebook (Colab environment)\n",
    "notebook_path = '/content/Copy_of_LSTM.ipynb'\n",
    "\n",
    "# Load the notebook\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove the 'widgets' metadata (this will strip all widgets)\n",
    "if 'widgets' in notebook.metadata:\n",
    "    del notebook.metadata['widgets']\n",
    "\n",
    "# Save the cleaned notebook back\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(notebook, f)\n",
    "\n",
    "print(\"All widget metadata removed from the notebook.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
